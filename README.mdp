# MayStreet Analytics Workbench Automated Task Springboard

## Welcome

Welcome to the Automated Task Springboard.

This Springboard will talk you through how to create an automated task which will, when triggered via a HTTP
request, retrieve a file from S3, run it against the Data Lake, and then write out another file.

## Prerequisites

- An external AWS account with a pre-created S3 bucket.
- Credentials (an AWS access key and secret key) that have read and write access to the above bucket.

## Preparing your Workbench instance.

1. You will need to create a Dask cluster before you can run this sample; to do so, please follow the short video
here:

![images/provisioning-cluster.gif](/images/provisioning-cluster.gif)

2. You will then need to create a simple web-triggered job using the cluster you've just created:

![images/creating-job.gif](/images/creating-job.gif)

3. Your Job edit should look like the following:

![job-config.png](/images/job-config.png)

## Running the job.

1. Ensure you have updated `BUCKET_NAME`, `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` in the _[augment.py](augment.py)_ file.
2. Create a file named '20-apr-2022.csv' with the following content inside that bucket:

| Product | Feed | Requested Date |
| ------- | ---- | -------------- |
| AAPL | bats_edga | 12/01/2022 |
| V | bats_edga | 13/01/2022 |
| V | bats_edga | 13/01/2022 |
| VRSN | bats_edga | 14/01/2022 |
| NOTHING | bats_false | 14/01/2022 |

(the file is also in this Springboard so you can easily just drop that in.)

3. Copy the job URL as show in the above image, and then customise it to add in the filename inside the bucket; the
command should look something like the following (with a different Job ID). Remember, if you need to get the URL then you can click the "copy" icon next to the name of the job in the Jobs view:

```
curl -H 'Content-Type: application/json' -X POST --data '{ "jobId": "<<UUID HERE>>", "data": {"filename":"20-apr-2022.csv" } }' https://wb-api.maystreet.com/api/dask-jobs/trigger
```

4. Wait, and you should see the file being generated in the S3 repository.

## Web APIs to Query Status

We offer a few APIs you can call to check the status of your executions.

To use these, you will need an API token. Open the Command Palette inside Workbench and select "MST: New API Key". Answer the questions,
and a new API key will be generated.

Keep this safe and secure, as anyone with it will be able to query jobs.

### Query all Executions

```
curl --location --request GET 'https://wb-api.maystreet.com/api/dask-jobs/my-executions' \
--header 'Authorization: Bearer <<TOKEN>>'
```

### Query a Single Execution

```
curl --location --request GET 'https://wb-api.maystreet.maystreet.com/api/dask-jobs/my-executions/<<UUID of Execution>>' \
--header 'Authorization: Bearer <<TOKEN>>'
```
